{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GnZdaTi6FgHf"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RVC - Disconnected**\n",
        "\n",
        "***Modified by [MedicDoesStuff](https://www.youtube.com/@medicdoesstuff/)***\n",
        "\n",
        "**Original notebook hastily written by [Kit Lemonfoot](https://huggingface.co/Kit-Lemonfoot) / [Noel Shirogane's High Flying Birds](https://www.youtube.com/@NoelShiroganesHighFlyingBirds)**\n",
        "\n",
        "*Based on the work of the [RVC Project](https://github.com/RVC-Project), [Mangio261](https://github.com/Mangio621/), [Kalomaze](https://github.com/kalomaze), the [Pony Preservation Project](https://boards.4channel.org/mlp/catalog#s=Pony%20Preservation%20Project), and many others in the RVC / voice AI community*\n",
        "\n",
        "*Notebook version:* **0.19.a**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook is designed to be used with training Retreival-Based Voice Conversion (RVC) models without using a WÐµbUl. This is done to stay within the scope of Colab's TOS.\n",
        "\n",
        "âš ï¸ WARNING: Unlike the original RVC notebook, *this notebook ***does not*** have autosave functionality* due to the massive amount of underlying stress that RVC's autosave features places on Colab to Drive communication. Please be careful and try to keep training sessions short."
      ],
      "metadata": {
        "id": "FrR4hoSR50Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dependencies**\n",
        "ðŸŸ¢ Run this first!"
      ],
      "metadata": {
        "id": "GnZdaTi6FgHf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBpJW3YB5zu1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies\n",
        "import subprocess\n",
        "\n",
        "packages = ['build-essential', 'python3-dev', 'ffmpeg', 'aria2']\n",
        "pip_packages = ['pip', 'setuptools', 'wheel', 'httpx==0.23.0', 'faiss-gpu', 'fairseq', 'ffmpeg', 'ffmpeg-python', 'praat-parselmouth', 'pyworld', 'numpy==1.23.5', 'numba==0.56.4', 'librosa==0.9.2', 'gdown', 'onnxruntime']\n",
        "print(\"Updating and installing system packages...\")\n",
        "for package in packages:\n",
        "  print(f\"Installing {package}...\")\n",
        "  subprocess.check_call(['apt-get', 'install', '-qq', '-y', package])\n",
        "\n",
        "print(\"Updating and installing pip packages...\")\n",
        "subprocess.check_call(['pip', 'install', '--upgrade'] + pip_packages)\n",
        "\n",
        "print('Packages up to date.')\n",
        "firsttry = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Clone Repositories\n",
        "import os\n",
        "\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/Mangio621/Mangio-RVC-Fork.git\n",
        "!git clone https://github.com/maxrmorrison/torchcrepe.git\n",
        "!mv torchcrepe/torchcrepe Mangio-RVC-Fork/\n",
        "!rm -rf torchcrepe  # Delete the torchcrepe repository folder\n",
        "os.chdir('/content/Mangio-RVC-Fork')\n",
        "\n",
        "now_dir = \"/content/Mangio-RVC-Fork\"\n",
        "os.makedirs(os.path.join(now_dir, \"logs\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(now_dir, \"weights\"), exist_ok=True)\n"
      ],
      "metadata": {
        "id": "McbFvkWfJQCO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GPU Check\n",
        "import torch\n",
        "\n",
        "ngpu = torch.cuda.device_count()\n",
        "gpu_infos = []\n",
        "mem = []\n",
        "if_gpu_ok = False\n",
        "\n",
        "if torch.cuda.is_available() or ngpu != 0:\n",
        "  for i in range(ngpu):\n",
        "    gpu_name = torch.cuda.get_device_name(i)\n",
        "    if any(\n",
        "        value in gpu_name.upper()\n",
        "        for value in [\"10\", \"16\", \"20\", \"30\", \"40\", \"A2\", \"A3\", \"A4\", \"P4\", \"A50\", \"500\", \"A60\", \"70\", \"80\", \"90\", \"M4\", \"T4\", \"TITAN\"]\n",
        "    ):\n",
        "      if_gpu_ok = True\n",
        "      print(\"Compatible GPU detected: %s\" % gpu_name)\n",
        "      gpu_infos.append(\"%s\\t%s\" % (i, gpu_name))\n",
        "      mem.append(int(torch.cuda.get_device_properties(i).total_memory / 1024 / 1024 / 1024 + 0.4))\n",
        "\n",
        "if if_gpu_ok and len(gpu_infos) > 0:\n",
        "  gpu_info = \"\\n\".join(gpu_infos)\n",
        "\n",
        "else:\n",
        "  raise Exception(\"No GPU detected; training cannot continue. Please change your runtime type to a GPU.\")\n",
        "gpus = \"-\".join(i[0] for i in gpu_infos)"
      ],
      "metadata": {
        "id": "E4W8p1ZLLXLy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "  drive.mount('/content/drive')\n",
        "else:\n",
        "  print('Drive is already mounted. Proceed.')\n",
        "\n",
        "os.makedirs('/content/drive/MyDrive/RVC-Disconnected', exist_ok=True)"
      ],
      "metadata": {
        "id": "1i1eYRMYfE79",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Pretrained Models\n",
        "#Didn't ask.\n",
        "\n",
        "#V1 Models\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0G32k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0D32k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0G40k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0D40k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0G48k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/v1/f0D48k.pth -d /content/Mangio-RVC-Fork/pretrained -o f0D48k.pth\n",
        "\n",
        "#V2 Models\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0G32k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0G32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0D32k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0D32k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0G40k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0G40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0D40k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0D40k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0G48k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0G48k.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/f0D48k.pth -d /content/Mangio-RVC-Fork/pretrained_v2 -o f0D48k.pth\n",
        "\n",
        "#RMVPE and Hubert\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/hubert_base.pt -d /content/Mangio-RVC-Fork -o hubert_base.pt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/rmvpe.pt -d /content/Mangio-RVC-Fork -o rmvpe.pt\n",
        "\n",
        "#fp_16 Variant JSONs\n",
        "!rm -rf /content/Mangio-RVC-Fork/configs/32k.json\n",
        "!rm -rf /content/Mangio-RVC-Fork/configs/40k.json\n",
        "!rm -rf /content/Mangio-RVC-Fork/configs/48k.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/32k.json -d /content/Mangio-RVC-Fork/configs -o 32k.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/40k.json -d /content/Mangio-RVC-Fork/configs -o 40k.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Kit-Lemonfoot/RVC_DidntAsk/resolve/main/48k.json -d /content/Mangio-RVC-Fork/configs -o 48k.json"
      ],
      "metadata": {
        "id": "v_zQGeguKD5s",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup CSVDB\n",
        "#...Alright, you made your point.\n",
        "import csv\n",
        "\n",
        "if not os.path.isdir(\"csvdb/\"):\n",
        "  os.makedirs(\"csvdb\")\n",
        "  frmnt, stp = open(\"csvdb/formanting.csv\", \"w\", newline=\"\"), open(\"csvdb/stop.csv\", \"w\", newline=\"\")\n",
        "  csv_writer = csv.writer(frmnt, delimiter=\",\")\n",
        "  csv_writer.writerow([False, 1.0, 1.0])\n",
        "  csv_writer = csv.writer(stp, delimiter=\",\")\n",
        "  csv_writer.writerow([False])\n",
        "  frmnt.close()\n",
        "  stp.close()\n",
        "\n",
        "global DoFormant, Quefrency, Timbre\n",
        "DoFormant, Quefrency, Timbre = False, 1.0, 1.0"
      ],
      "metadata": {
        "cellView": "form",
        "id": "0LGNhYjGaBBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Set Training Variables**\n",
        "\n",
        "- `experiment_name`: The name of the model. (i.e: EminemKamikaze)\n",
        "- `model_architecture`: Specifies the chosen model version. V2 is recommended.\n",
        "- `target_sample_rate`: Specifies the desired sample rate for the model, 40K is recommended for general training.\n",
        "- `cpu_threads`: Specifies the number of CPU threads to be used during the training. Colab only has 2 so don't even bother changing it.\n",
        "- `speaker_id`: Represents the ID of the speaker being trained, I recommend not changing it.\n",
        "- `pitch_extraction_algorithm`: Specifies the algorithm used for pitch extraction from the audio data, rmvpe is recommended.\n",
        "- `crepe_hop_length`: Represents the hop length parameter used in the Crepe algorithm for pitch extraction, only affects crepe and mangio-crepe.\n",
        "- `pitch_guidance`: Indicates whether pitch guidance is enabled or disabled for the experiment, can be useful if you want a talking or singing model."
      ],
      "metadata": {
        "id": "oi6yLWNM31rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#title Set Training Variables (Run Every Time)\n",
        "now_dir = \"/content/Mangio-RVC-Fork\"\n",
        "experiment_name = \"experiment_name\" #@param {type:\"string\"}\n",
        "path_to_training_folder = \"/content/dataset/\"\n",
        "model_architecture = \"v2\" #@param [\"v1\",\"v2\"] {allow-input: false}\n",
        "target_sample_rate = \"40k\" #@param [\"32k\", \"40k\", \"48k\"] {allow-input: false}\n",
        "cpu_threads = 2 #@param {type:\"integer\"}\n",
        "speaker_id = 0 #@param {type:\"integer\"}\n",
        "pitch_extraction_algorithm = \"rmvpe\" #@param [\"harvest\", \"crepe\", \"mangio-crepe\", \"rmvpe\"] {allow-input: false}\n",
        "crepe_hop_length = 64 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "pitch_guidance = True #@param {type:\"boolean\"}\n",
        "\n",
        "if(experiment_name == \"experiment_name\"):\n",
        "  print(\"Warning: Your experiment name should be changed to the name of your dataset.\")\n",
        "\n",
        "if(experiment_name.find(\" \")!=-1):\n",
        "  experiment_name = experiment_name.replace(\" \", \"_\")\n",
        "  print(\"I detected spaces in your experiment name, which should not be used. These spaces have been replaced with underscores.\")\n",
        "  print(\"Your new experiment name is: \"+experiment_name)"
      ],
      "metadata": {
        "id": "ZodNcumpg-JM",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocessing**\n",
        "You should only need to run these cells once per model."
      ],
      "metadata": {
        "id": "GpjFLdlRFlcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load dataset\n",
        "#@markdown If it doesn't already exist, create a folder in your Google Drive named 'RVC-Disconnected' and place your zip file there. This will look for the following ZIP file inside that 'RVC-Disconnected' folder.\n",
        "dataset = \"zipfile.zip\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown This loader will load datasets in a similar fashion to the So-Vits-SVC dataset loader. For best results, your dataset should be formatted as such:\n",
        "#@markdown ```\n",
        "#@markdown zipfile.zip\n",
        "#@markdown â””â”€â”€â”€character_name\n",
        "#@markdown     â”œâ”€â”€â”€file1.wav\n",
        "#@markdown     â”œâ”€â”€â”€...\n",
        "#@markdown     â””â”€â”€â”€file999.wav\n",
        "#@markdown ```\n",
        "#@markdown Audio filenames do not matter. All audio files should be in WAV format for best compatibility.\n",
        "\n",
        "# TODO: Add something to convert non-WAVs to WAV\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "directories=[]\n",
        "\n",
        "def sanitize_directory(directory):\n",
        "  for filename in os.listdir(directory):\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "      if filename == \".DS_Store\" or filename.startswith(\"._\") or not filename.endswith(('.wav', '.flac', '.mp3', '.ogg', '.m4a')):\n",
        "        os.remove(file_path)\n",
        "    elif os.path.isdir(file_path):\n",
        "      #Get rid of the MACOSX directory just so it doesn't mess with renaming later\n",
        "      if(filename == \"__MACOSX\"):\n",
        "        shutil.rmtree(file_path)\n",
        "        continue\n",
        "      #Append the directory to directories for future dataset check, then recurse.\n",
        "      directories.append(file_path)\n",
        "      sanitize_directory(file_path)\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/RVC-Disconnected/' + dataset\n",
        "final_directory = '/content/dataset'\n",
        "temp_directory = '/content/temp_dataset'\n",
        "\n",
        "if os.path.exists(final_directory):\n",
        "  print(\"Dataset folder already found. Wiping...\")\n",
        "  shutil.rmtree(final_directory)\n",
        "if os.path.exists(temp_directory):\n",
        "  print(\"Temporary folder already found. Wiping...\")\n",
        "  shutil.rmtree(temp_directory)\n",
        "\n",
        "if not os.path.exists(dataset_path):\n",
        "  raise Exception(f'I can\\'t find {dataset} in {os.path.dirname(dataset_path)}.')\n",
        "\n",
        "os.makedirs(final_directory, exist_ok=True)\n",
        "os.makedirs(temp_directory, exist_ok=True)\n",
        "#Oops.\n",
        "!unzip -d {temp_directory} -B \"{dataset_path}\"\n",
        "print(\"Sanitizing...\")\n",
        "sanitize_directory(temp_directory)\n",
        "\n",
        "if(len(directories) == 0):\n",
        "  #If there's no directories, we're dealing with a ZIP of just audio files.\n",
        "  #Move everything to /dataset/experiment_name/.\n",
        "  print(\"Dataset Type: Audio Files (Single Speaker)\")\n",
        "  expDir=os.path.join(final_directory, experiment_name)\n",
        "  os.makedirs(expDir, exist_ok=True)\n",
        "  for r, _, f in os.walk(temp_directory):\n",
        "    for name in f:\n",
        "      !cp {temp_directory}/{name} {expDir}\n",
        "elif(len(directories) == 1):\n",
        "  #If there's only one directory, we're dealing with a single speaker.\n",
        "  #Rename the folder to experiment_name and move it to /dataset/.\n",
        "  print(\"Dataset Type: Single Speaker\")\n",
        "  fi = os.path.join(temp_directory, experiment_name)\n",
        "  os.rename(directories[0], fi)\n",
        "  shutil.move(fi, final_directory)\n",
        "\n",
        "else:\n",
        "  #If anything else, we're dealing with multispeaker.\n",
        "  #Move all folders to /dataset/ indiscriminately.\n",
        "  print(\"Dataset Type: Multispeaker\")\n",
        "  for fi in directories:\n",
        "    shutil.move(fi, final_directory)\n",
        "\n",
        "shutil.rmtree(temp_directory)\n",
        "\n",
        "print(\"Dataset imported.\")\n"
      ],
      "metadata": {
        "id": "g1X5oANker6l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocessing\n",
        "# Change the Experiment Name and the Path to Training Folder. You shouldn't need to change anything else.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "assert cpu_threads>0, \"CPU threads not allocated correctly.\"\n",
        "\n",
        "sr = int(target_sample_rate.rstrip('k'))*1000\n",
        "pttf = path_to_training_folder + experiment_name\n",
        "os.makedirs(\"%s/logs/%s\" % (now_dir, experiment_name), exist_ok=True)\n",
        "\n",
        "cmd = \"python trainset_preprocess_pipeline_print.py %s %s %s %s/logs/%s 1\" % (pttf, sr, cpu_threads, now_dir, experiment_name)\n",
        "print(cmd)\n",
        "!$cmd"
      ],
      "metadata": {
        "id": "OPNzuVYG7N_R",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Feature extraction\n",
        "\n",
        "pitch_extraction_algorithm = \"rmvpe\" #@param [\"harvest\", \"crepe\", \"mangio-crepe\", \"rmvpe\"] {allow-input: false}\n",
        "#crepe_hop_length = 128 #@param {type:\"slider\", min:1, max:512, step:1}\n",
        "#pitch_guidance = True #@param {type:\"boolean\"}\n",
        "\n",
        "gpuList = gpus.split(\"-\")\n",
        "cmd = \"python extract_f0_print.py %s/logs/%s %s %s %s\" % (now_dir, experiment_name, cpu_threads, pitch_extraction_algorithm, crepe_hop_length)\n",
        "print(cmd)\n",
        "!$cmd\n",
        "\n",
        "leng = len(gpus)\n",
        "\n",
        "cmd = \"python extract_feature_print.py %s %s %s %s %s/logs/%s %s\" % (\"device\", leng, 0, 0, now_dir, experiment_name, model_architecture)\n",
        "print(cmd)\n",
        "!$cmd\n"
      ],
      "metadata": {
        "id": "u6iN6C-U94DS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save preprocessed dataset files to Google Drive\n",
        "#Compress dataset folder\n",
        "loc = \"%s/logs/%s\" % (now_dir, experiment_name)\n",
        "!zip -r rvcLogs.zip {loc}\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/RVC-Disconnected/\" + experiment_name\n",
        "!mkdir -p {DATASET_PATH_DRIVE}\n",
        "!cp /content/Mangio-RVC-Fork/rvcLogs.zip {DATASET_PATH_DRIVE}"
      ],
      "metadata": {
        "id": "y959_sq7kToW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**\n",
        "Also includes resuming code."
      ],
      "metadata": {
        "id": "vjcmh9u_MweU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load preprocessed dataset files from Google Drive (for resuming)\n",
        "#@markdown If you already have preprocessed dataset files on Google Drive, you can load them here instead of re-running the preprocessing steps.\n",
        "import os\n",
        "\n",
        "BACK_UP_DATASET_PATH = \"/content/drive/MyDrive/RVC-Disconnected/\" + experiment_name\n",
        "\n",
        "#Prevent people from loading the ZIP over existing files\n",
        "ok=True\n",
        "if(os.path.exists(\"/content/Mangio-RVC-Fork/logs/\"+experiment_name+\"/2a_f0\")):\n",
        "  print(\"Dataset files already loaded, skipping.\")\n",
        "  ok=False\n",
        "\n",
        "if ok:\n",
        "  !unzip {BACK_UP_DATASET_PATH}/rvcLogs.zip -d /"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_BkhWhrCMdgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Model from Drive to Notebook (for resuming)\n",
        "import os\n",
        "\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/RVC-Disconnected/\" + experiment_name\n",
        "DATASET_PATH_COLAB = \"/content/Mangio-RVC-Fork/logs/\" + experiment_name\n",
        "#@markdown Input the model's Step Count here (the number located on your model's G and D files.) If you used `save_only_last_ckpt` during training, this number will be 2333333.\n",
        "STEPCOUNT = 2333333 #@param {type:\"integer\"}\n",
        "\n",
        "print(\"Copying model files...\")\n",
        "!cp {DATASET_PATH_DRIVE}/D_{STEPCOUNT}.pth {DATASET_PATH_COLAB}\n",
        "!cp {DATASET_PATH_DRIVE}/G_{STEPCOUNT}.pth {DATASET_PATH_COLAB}\n",
        "!cp {DATASET_PATH_DRIVE}/config.json {DATASET_PATH_COLAB}\n",
        "\n",
        "print(\"Copying Tensorboard TFEVENT files...\")\n",
        "for r, _, f in os.walk(DATASET_PATH_DRIVE):\n",
        "  for name in f:\n",
        "    if(name.startswith(\"events.out.tfevents\")):\n",
        "      !cp {DATASET_PATH_DRIVE}/{name} {DATASET_PATH_COLAB}\n",
        "\n",
        "print(\"All done. Welcome back!\")"
      ],
      "metadata": {
        "id": "JbmAq_mZ7Zuh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "from random import shuffle\n",
        "\n",
        "# I will not be adding an autosave feature. Do not ask.\n",
        "\n",
        "#@title Training\n",
        "save_frequency = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "total_epochs = 300 #@param {type:\"slider\", min:1, max:10000, step:1}\n",
        "batch_size = 12 #@param {type:\"slider\", min:1, max:40, step:1}\n",
        "save_only_latest_ckpt = True #@param {type:\"boolean\"}\n",
        "cache_all_training_sets = False #@param {type:\"boolean\"}\n",
        "save_small_final_model = True #@param {type:\"boolean\"}\n",
        "#@markdown The automatically calculated log interval is known to be very inaccurate and can cause delays between an epoch finishing and Tensorboard writes. If you would like, you can manually define a log interval here.\n",
        "use_manual_stepToEpoch = False #@param {type:\"boolean\"}\n",
        "manual_stepToEpoch = 000 #@param {type:\"integer\"}\n",
        "\n",
        "pretrained_base = \"pretrained/\" if model_architecture == \"v1\" else \"pretrained_v2/\"\n",
        "exp_dir = \"%s/logs/%s\" % (now_dir, experiment_name)\n",
        "\n",
        "pretrainedD = \"%sf0D%s.pth\" % (pretrained_base, target_sample_rate)\n",
        "pretrainedG = \"%sf0G%s.pth\" % (pretrained_base, target_sample_rate)\n",
        "\n",
        "#Log interval\n",
        "log_interval = 1\n",
        "liFolderPath = os.path.join(exp_dir, \"1_16k_wavs\")\n",
        "if(os.path.exists(liFolderPath) and os.path.isdir(liFolderPath)):\n",
        "  wav_files = [f for f in os.listdir(liFolderPath) if f.endswith(\".wav\")]\n",
        "  if wav_files:\n",
        "    sample_size = len(wav_files)\n",
        "    log_interval = math.ceil(sample_size / batch_size)\n",
        "    if log_interval > 1:\n",
        "      log_interval += 1\n",
        "\n",
        "if log_interval > 250 and not use_manual_stepToEpoch:\n",
        "  print(\"That's a big dataset you got there. Log interval normalized to 200 steps from %s steps.\" % log_interval)\n",
        "  log_interval = 200\n",
        "\n",
        "if use_manual_stepToEpoch:\n",
        "  log_interval = manual_stepToEpoch\n",
        "\n",
        "#Create Python command\n",
        "cmd = \"python train_nsf_sim_cache_sid_load_pretrain.py -e %s -sr %s -f0 %s -bs %s -g %s -te %s -se %s %s %s -l %s -c %s -sw %s -v %s -li %s\" % (\n",
        "    experiment_name,\n",
        "    target_sample_rate,\n",
        "    1,\n",
        "    batch_size,\n",
        "    0,\n",
        "    total_epochs,\n",
        "    save_frequency,\n",
        "    \"-pg %s\" % pretrainedG if pretrainedG != \"\" else \"\\b\",\n",
        "    \"-pd %s\" % pretrainedD if pretrainedD != \"\" else \"\\b\",\n",
        "    1 if save_only_latest_ckpt else 0,\n",
        "    1 if cache_all_training_sets else 0,\n",
        "    1 if save_small_final_model else 0,\n",
        "    model_architecture,\n",
        "    log_interval,\n",
        ")\n",
        "print(cmd)\n",
        "\n",
        "#Create mute filelist\n",
        "exp_dir = \"%s/logs/%s\" % (now_dir, experiment_name)\n",
        "gt_wavs_dir = \"%s/0_gt_wavs\" % (exp_dir)\n",
        "feature_dir = (\n",
        "  \"%s/3_feature256\" % (exp_dir)\n",
        "  if model_architecture == \"v1\"\n",
        "  else \"%s/3_feature768\" % (exp_dir)\n",
        ")\n",
        "f0_dir = \"%s/2a_f0\" % (exp_dir)\n",
        "f0nsf_dir = \"%s/2b-f0nsf\" % (exp_dir)\n",
        "names = (\n",
        "  set([name.split(\".\")[0] for name in os.listdir(gt_wavs_dir)])\n",
        "  & set([name.split(\".\")[0] for name in os.listdir(feature_dir)])\n",
        "  & set([name.split(\".\")[0] for name in os.listdir(f0_dir)])\n",
        "  & set([name.split(\".\")[0] for name in os.listdir(f0nsf_dir)])\n",
        ")\n",
        "opt = []\n",
        "for name in names:\n",
        "  opt.append(\n",
        "    \"%s/%s.wav|%s/%s.npy|%s/%s.wav.npy|%s/%s.wav.npy|%s\"\n",
        "    % (\n",
        "      gt_wavs_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "      name,\n",
        "      feature_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "      name,\n",
        "      f0_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "      name,\n",
        "      f0nsf_dir.replace(\"\\\\\", \"\\\\\\\\\"),\n",
        "      name,\n",
        "      speaker_id,\n",
        "    )\n",
        "  )\n",
        "fea_dim = 256 if model_architecture == \"v1\" else 768\n",
        "for _ in range(2):\n",
        "  opt.append(\n",
        "      \"%s/logs/mute/0_gt_wavs/mute%s.wav|%s/logs/mute/3_feature%s/mute.npy|%s/logs/mute/2a_f0/mute.wav.npy|%s/logs/mute/2b-f0nsf/mute.wav.npy|%s\"\n",
        "      % (now_dir, target_sample_rate, now_dir, fea_dim, now_dir, now_dir, speaker_id)\n",
        "  )\n",
        "shuffle(opt)\n",
        "with open(\"%s/filelist.txt\" % exp_dir, \"w\") as f:\n",
        "  f.write(\"\\n\".join(opt))\n",
        "print(\"Mute filelist written. Best of luck training!\")\n",
        "\n",
        "\n",
        "%cd /content/Mangio-RVC-Fork\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/Mangio-RVC-Fork/logs\n",
        "\n",
        "os.chdir('/content/Mangio-RVC-Fork')\n",
        "!$cmd\n"
      ],
      "metadata": {
        "id": "Yj-_npuA_HRB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export Model from Notebook to Drive\n",
        "import os\n",
        "\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/RVC-Disconnected/\" + experiment_name\n",
        "DATASET_PATH_COLAB = \"/content/Mangio-RVC-Fork/logs/\" + experiment_name\n",
        "if(not os.path.exists(DATASET_PATH_DRIVE)):\n",
        "  !mkdir -p {DATASET_PATH_DRIVE}\n",
        "\n",
        "#@markdown Use this option if you wish to only copy over weights.\n",
        "skip_models = False #@param {type:\"boolean\"}\n",
        "#@markdown Use these options if you wish to manually input your step count and epoch count for incomplete models. *Do not use this option if your training finished.*\n",
        "manual_save = False #@param {type:\"boolean\"}\n",
        "STEPCOUNT = 000 #@param {type:\"integer\"}\n",
        "EPOCHCOUNT = 000 #@param {type:\"integer\"}\n",
        "\n",
        "finished=False\n",
        "potential=\"/content/Mangio-RVC-Fork/weights/\"+experiment_name+\".pth\"\n",
        "if os.path.exists(potential):\n",
        "  finished = True\n",
        "\n",
        "#VERY hacky. Might break stuff, report to me if it does.\n",
        "print(\"Detecting latest model...\")\n",
        "if(not manual_save):\n",
        "  currentMax = 0\n",
        "  for r, _, f in os.walk(\"/content/Mangio-RVC-Fork/weights/\"):\n",
        "    for name in f:\n",
        "      if(name.endswith(\".pth\") and (name!=experiment_name+\".pth\")):\n",
        "        #Check to see if this PTH is what we're looking for.\n",
        "        if(name.find(experiment_name)==-1):\n",
        "          continue\n",
        "        #Determine Epochcount+Stepcount Phase 1\n",
        "        pot = name.split('_')\n",
        "        ep=pot[len(pot)-2][1:]\n",
        "        #If what we got from the epoch count section of the filename isn't a number, multiple completed models are in weights.\n",
        "        #Skip it if that happens.\n",
        "        if(not ep.isdecimal()):\n",
        "          continue\n",
        "        #Determine Epochcount+Stepcount Phase 2\n",
        "        ep=int(ep)\n",
        "        if ep>currentMax:\n",
        "          currentMax=ep\n",
        "          step=pot[len(pot)-1].split('.')\n",
        "          step=int(step[0][1:])\n",
        "          EPOCHCOUNT=ep\n",
        "          STEPCOUNT=step\n",
        "\n",
        "TSTEP = STEPCOUNT\n",
        "if(not skip_models):\n",
        "  print(\"Copying model files...\")\n",
        "  if(save_only_latest_ckpt):\n",
        "    TSTEP=2333333\n",
        "  !cp {DATASET_PATH_COLAB}/D_{TSTEP}.pth \"{DATASET_PATH_DRIVE}\"\n",
        "  !cp {DATASET_PATH_COLAB}/G_{TSTEP}.pth \"{DATASET_PATH_DRIVE}\"\n",
        "  !cp {DATASET_PATH_COLAB}/config.json \"{DATASET_PATH_DRIVE}\"\n",
        "\n",
        "print(\"Copying Tensorboard TFEVENT files...\")\n",
        "for r, d, f in os.walk(DATASET_PATH_COLAB):\n",
        "  for name in f:\n",
        "    if(name.startswith(\"events.out.tfevents\") and os.path.exists(os.path.join(DATASET_PATH_COLAB, name))):\n",
        "      !cp {DATASET_PATH_COLAB}/{name} {DATASET_PATH_DRIVE}\n",
        "\n",
        "print(\"Copying weight file...\")\n",
        "if(finished):\n",
        "  !cp {potential} \"{DATASET_PATH_DRIVE}\"\n",
        "else:\n",
        "  !cp /content/Mangio-RVC-Fork/weights/{experiment_name}_e{EPOCHCOUNT}_s{STEPCOUNT}.pth \"{DATASET_PATH_DRIVE}\"\n",
        "\n",
        "print(\"All done!\")"
      ],
      "metadata": {
        "id": "HDsTxpbTqHol",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Index training\n",
        "#@markdown Ensure that Feature Extraction has run successfully before running this cell.\n",
        "\n",
        "#@markdown **Note: It's rare, but you may encounter a bug with this cell that requires a runtime restart.**\n",
        "#@markdown **If this happens, restart the runtime, re-run the \"Set Training Variables\" cell, then re-run this cell.**\n",
        "\n",
        "#@markdown Use this option if you wish to save the two extra files generated by index training to your Google Drive. (Only the added index is normally needed.)\n",
        "save_extra_files_to_drive = False #@param {type:\"boolean\"}\n",
        "\n",
        "#Oh dear lord why is this baked into infer-web I hate this\n",
        "import os\n",
        "import sys\n",
        "import traceback\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "#from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "exp_dir = \"%s/logs/%s\" % (now_dir, experiment_name)\n",
        "os.makedirs(exp_dir, exist_ok=True)\n",
        "feature_dir = (\n",
        "    \"%s/3_feature256\" % (exp_dir)\n",
        "    if model_architecture == \"v1\"\n",
        "    else \"%s/3_feature768\" % (exp_dir)\n",
        ")\n",
        "print(feature_dir)\n",
        "if not os.path.exists(feature_dir):\n",
        "  raise Exception(\"No features exist for this model yet. Did you run Feature Extraction?\")\n",
        "listdir_res = list(os.listdir(feature_dir))\n",
        "if len(listdir_res) == 0:\n",
        "  raise Exception(\"No features exist for this model yet. Did you run Feature Extraction?\")\n",
        "\n",
        "try:\n",
        "  from sklearn.cluster import MiniBatchKMeans\n",
        "except:\n",
        "  print(\"Due to a bug with Colab, we will need to reinstall Numpy real quick. Give me a sec!\")\n",
        "  !pip install -U numpy\n",
        "  print(\"Numpy reinstalled. Please restart the runtime, and then re-run the \\\"Set Training Variables\\\" cell to continue.\")\n",
        "  sys.exit()\n",
        "else:\n",
        "  print(\"Proper Numpy version detected.\")\n",
        "\n",
        "infos=[]\n",
        "npys=[]\n",
        "for name in sorted(listdir_res):\n",
        "  phone = np.load(\"%s/%s\" % (feature_dir, name))\n",
        "  npys.append(phone)\n",
        "big_npy = np.concatenate(npys, 0)\n",
        "big_npy_idx = np.arange(big_npy.shape[0])\n",
        "np.random.shuffle(big_npy_idx)\n",
        "if big_npy.shape[0] > 2e5:\n",
        "  print(\"Trying doing kmeans %s shape to 10k centers.\" % big_npy.shape[0])\n",
        "  try:\n",
        "    big_npy = (\n",
        "        MiniBatchKMeans(\n",
        "            n_clusters=10000,\n",
        "            verbose=True,\n",
        "            batch_size=256,\n",
        "            compute_labels = False,\n",
        "            init=\"random\"\n",
        "        )\n",
        "        .fit(big_npy)\n",
        "        .cluster_centers_\n",
        "\n",
        "    )\n",
        "  except:\n",
        "    info = traceback.format_exc()\n",
        "    print(info)\n",
        "\n",
        "np.save(\"%s/total_fea.npy\" % exp_dir, big_npy)\n",
        "n_ivf = min(int(16*np.sqrt(big_npy.shape[0])), big_npy.shape[0] // 39)\n",
        "print(\"%s,%s\" % (big_npy.shape, n_ivf))\n",
        "index = faiss.index_factory(256 if model_architecture == \"v1\" else 768, \"IVF%s,Flat\" % n_ivf)\n",
        "print(\"Training index...\")\n",
        "index_ivf = faiss.extract_index_ivf(index)\n",
        "index_ivf.nprobe = 1\n",
        "index.train(big_npy)\n",
        "faiss.write_index(\n",
        "    index,\n",
        "    \"%s/trained_IVF%s_Flat_nprobe_%s_%s_%s.index\" % (exp_dir, n_ivf, index_ivf.nprobe, experiment_name, model_architecture)\n",
        ")\n",
        "print(\"Adding...\")\n",
        "batch_size_add = 8192\n",
        "for i in range(0, big_npy.shape[0], batch_size_add):\n",
        "  index.add(big_npy[i:i+batch_size_add])\n",
        "faiss.write_index(\n",
        "    index,\n",
        "    \"%s/added_IVF%s_Flat_nprobe_%s_%s_%s.index\"\n",
        "    % (exp_dir, n_ivf, index_ivf.nprobe, experiment_name, model_architecture)\n",
        ")\n",
        "\n",
        "npr = index_ivf.nprobe\n",
        "\n",
        "print(\"Saving files to Drive...\")\n",
        "DATASET_PATH_DRIVE = \"/content/drive/MyDrive/RVC-Disconnected/\" + experiment_name\n",
        "if(not os.path.exists(DATASET_PATH_DRIVE)):\n",
        "  !mkdir -p {DATASET_PATH_DRIVE}\n",
        "DATASET_PATH_COLAB = \"/content/Mangio-RVC-Fork/logs/\" + experiment_name\n",
        "if(save_extra_files_to_drive):\n",
        "  !cp {DATASET_PATH_COLAB}/total_fea.npy \"{DATASET_PATH_DRIVE}\"\n",
        "  !cp {DATASET_PATH_COLAB}/trained_IVF{n_ivf}_Flat_nprobe_{npr}_{experiment_name}_{model_architecture}.index \"{DATASET_PATH_DRIVE}\"\n",
        "!cp {DATASET_PATH_COLAB}/added_IVF{n_ivf}_Flat_nprobe_{npr}_{experiment_name}_{model_architecture}.index \"{DATASET_PATH_DRIVE}\"\n",
        "\n",
        "print(\"All done! Your index file has completed training.\")\n",
        "try:\n",
        "  firsttry\n",
        "except:\n",
        "  print(\"If you had to restart the runtime, disconnect and delete the runtime in order to continue. (Restarting the runtime again will not work.)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NYDdsNhx4Qdi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TODO, Changelogs, Credits, and Special Thanks**\n",
        "**TODO**\n",
        "*   Continue to check for issues with the model exporter. The current non-manual method is very hacky and is prone to errors.\n",
        "*   Add a WAV converter to the dataset loader\n",
        "*   Implement \"Export Lowest Points\", or implement [ROD](https://github.com/grvyscale/RealtimeOvertrainingDetection) on a seperate thread\n",
        "*   Remove some of the magic numbers from Feature Extraction (may not be needed, dunno if Colab has support for multiple GPUs anyways)\n",
        "\n",
        "**Changelogs**\n",
        "\n",
        "**v0.19** - Added some text to Import Model From Drive to better explain its' function.\n",
        "\n",
        "Rearranged some cells to make resuming processes a little clearer.\n",
        "\n",
        "Added a custom JSON import to force RVC to use fp16. *This may break stuff, report to me if it does.*\n",
        "\n",
        "**v0.18** - Hotfix: Fixed multiple bugs with Export Model in regards to multiple completed models being in the Weights folder. Models that are not the experiment name are now ignored.\n",
        "\n",
        "**v0.17** - Fixed a bug with Import Dataset that would fail to extract ZIP files with spaces in its' name.\n",
        "\n",
        "Fixed a bug with Import Model that would attempt to look for the model in the wrong place. Oops.\n",
        "\n",
        "**v0.16** - Fixed a bug with Export Model where models trained with `save_only_latest_ckpt` would not be saved to Drive properly.\n",
        "\n",
        "Fixed a bug with Index Training and Export Model that glitched out saving if `experiment_name` folder did not exist on Drive yet from preprocess saving. These cells will now create the folder if it detects it does not exist.\n",
        "\n",
        "Reverted a change in Export Model from 0.14 that prevented finished models from being saved.\n",
        "\n",
        "**v0.15** - Reworked the CSVDB setup to remove the early CSVUtil reliancy. This should hopefully fix a rare bug that causes Colab to error out while creating the dummy formanting CSV files.\n",
        "\n",
        "**v0.14** - Hotfix: Fixed an issue with Export Model where it would error out on model names with underscores. Hopefully.\n",
        "\n",
        "**v0.13** - Hotfix: Added an experimental autodetector for Export Model.\n",
        "\n",
        "Fixed a long-standing bug that would cause useless errors when transferring TFEVENT files out of a notebook.\n",
        "\n",
        "**v0.12** - Reworked the dataset loader to accept more forms of datasets.\n",
        "\n",
        "**v0.11** - Hotfix: Keep preprocessor ZIP loader from double-loads, added a warning regarding unchanged experiment_name, properly linked to the PPP in credits\n",
        "\n",
        "**v0.1** - Initial release\n",
        "\n",
        "**Credits**\n",
        "*   [MedicDoesStuff](https://www.youtube.com/@medicdoesstuff/) - Modified the notebook\n",
        "*   [Kit Lemonfoot](https://huggingface.co/Kit-Lemonfoot) - created the original No-WebUI notebook.\n",
        "*   [RVC Project](https://github.com/RVC-Project) - Created RVC, obviously\n",
        "*   [LJ1995](https://huggingface.co/lj1995) - Pretrained RVC models\n",
        "*   [Mangio261](https://github.com/Mangio621/) - Creating the Mangio RVC fork\n",
        "*   [Kalomaze](https://github.com/kalomaze) - Original RVC colab + Mangio tweaks\n",
        "*   [Pony Preservation Project](https://boards.4channel.org/mlp/catalog#s=Pony%20Preservation%20Project) - for their robust TalkNet and So-Vits-SVC notebooks\n",
        "*   the Colab team - forcing my hand and making me release this notebook early\n",
        "\n",
        "**Special Thanks**\n",
        "*   [Fifteen AI](https://15.ai/) - for getting me into voice AI in the first place\n",
        "*   [Dacoolkid44](https://huggingface.co/dacoolkid44) / [HoloAI44](https://www.youtube.com/@Holo_AI44) and Hijack / [SANSSWEEP](https://huggingface.co/SANSSWEEP) - for basically kickstarting the larger VTuber voice AI scene with their models\n",
        "*   [Maki Ligon](https://www.youtube.com/@Shiina_Mashiro) / [Yuuto Ichika](https://www.youtube.com/@yuutoichika) - for keeping me grounded in reality while developing this thing\n",
        "*   [Bartezes](https://www.youtube.com/@bartezes3082) - for helping me so much with the [VTuber AI Model Tracking spreadsheet](https://docs.google.com/spreadsheets/d/1tvZSggOsZGAPjbMrWOAAaoJJFpJuQlwUEQCf5x1ssO8/)\n",
        "*   [Megaaziib](https://www.youtube.com/@megaaziib) - for inspiring me to keep working on AI models and covers (I don't hate you)\n",
        "*   [Saintlysaint](https://www.youtube.com/@SaintlySaint) and [Gengar2525](https://www.youtube.com/@GeGaCh) - for having faith in me\n"
      ],
      "metadata": {
        "id": "g3fR68Yfkayg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**UVR Isolation Stuff**"
      ],
      "metadata": {
        "id": "r5Pj5Lid9kvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initialised = True\n",
        "from time import sleep\n",
        "from google.colab import output\n",
        "from google.colab import drive\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import glob\n",
        "\n",
        "mount_to_drive = True\n",
        "mount_path = '/content/drive/MyDrive'\n",
        "\n",
        "ai = 'https://github.com/kae0-0/Colab-for-MDX_B'\n",
        "ai_version = 'https://github.com/kae0-0/Colab-for-MDX_B/raw/main/v'\n",
        "onnx_list = 'https://raw.githubusercontent.com/kae0-0/Colab-for-MDX_B/main/onnx_list'\n",
        "#@title Initialize UVR MDX-Net Models\n",
        "#@markdown The 'ForceUpdate' option will update the models by fully reinstalling.\n",
        "ForceUpdate = False #@param {type:\"boolean\"}\n",
        "class h:\n",
        "    def __enter__(self):\n",
        "        self._original_stdout = sys.stdout\n",
        "        sys.stdout = open(os.devnull, 'w')\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        sys.stdout.close()\n",
        "        sys.stdout = self._original_stdout\n",
        "def get_size(bytes, suffix='B'): # read ram\n",
        "    global svmem\n",
        "    factor = 1024\n",
        "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n",
        "        if bytes < factor:\n",
        "            return f'{bytes:.2f}{unit}{suffix}'\n",
        "        bytes /= factor\n",
        "    svmem = psutil.virtual_memory()\n",
        "def console(t):\n",
        "    get_ipython().system(t)\n",
        "def LinUzip(file): # unzip call linux, force replace\n",
        "    console(f'unzip -o {file}')\n",
        "#-------------------------------------------------------\n",
        "def getONNX():\n",
        "    console(f'wget {onnx_list} -O onnx_list')\n",
        "    _onnx = open(\"onnx_list\", \"r\")\n",
        "    _onnx = _onnx.readlines()\n",
        "    os.remove('onnx_list')\n",
        "    for model in _onnx:\n",
        "        _model = sanitize_filename(os.path.basename(model))\n",
        "        console(f'wget {model}')\n",
        "        LinUzip(_model)\n",
        "        os.remove(_model)\n",
        "\n",
        "def getDemucs(_path):\n",
        "    #https://dl.fbaipublicfiles.com/demucs/v3.0/demucs_extra-3646af93.th\n",
        "    root = \"https://dl.fbaipublicfiles.com/demucs/v3.0/\"\n",
        "    model = {\n",
        "        'demucs_extra': '3646af93'\n",
        "    }\n",
        "    for models in zip(model.keys(),model.values()):\n",
        "        console(f'wget {root+models[0]+\"-\"+models[1]}.th -O {models[0]}.th')\n",
        "    for _ in glob.glob('*.th'):\n",
        "        if os.path.isfile(os.path.join(os.getcwd(),_path,_)):\n",
        "            os.remove(os.path.join(os.getcwd(),_path,_))\n",
        "        shutil.move(_,_path)\n",
        "\n",
        "def mount(gdrive=False):\n",
        "    if gdrive:\n",
        "        if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "            try:\n",
        "                drive.mount(\"/content/drive\", force_remount=True)\n",
        "            except:\n",
        "                drive._mount(\"/content/drive\", force_remount=True)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "mount(mount_to_drive)\n",
        "\n",
        "def toPath(path='local'):\n",
        "    if path == 'local':\n",
        "        os.chdir('/content')\n",
        "    elif path == 'gdrive':\n",
        "        os.chdir(mount_path)\n",
        "\n",
        "def update():\n",
        "    with h():\n",
        "        console(f'wget {ai_version} -O nver')\n",
        "    f = open('nver', 'r')\n",
        "    nver = f.read()\n",
        "    f = open('v', 'r')\n",
        "    cver = f.read()\n",
        "    if nver != cver or ForceUpdate:\n",
        "        print('New update found! {}'.format(nver))\n",
        "        os.chdir('../')\n",
        "        print('Updating ai...',end=' ')\n",
        "        with h():\n",
        "            console(f'git clone {ai} temp_MDX_Colab')\n",
        "            console('cp -a temp_MDX_Colab/* MDX_Colab/')\n",
        "            console('rm -rf temp_MDX_Colab')\n",
        "        print('done')\n",
        "        os.chdir('MDX_Colab')\n",
        "        print('Refreshing models...', end=' ')\n",
        "        with h():\n",
        "            #getDemucs('model/')\n",
        "            getONNX()\n",
        "        print('done')\n",
        "        output.clear()\n",
        "        os.remove('v')\n",
        "        os.rename(\"nver\",'v')\n",
        "        #os.chdir(f'{os.path.join(mount_path,\"MDX_Colab\")}')\n",
        "    else:\n",
        "        os.remove('nver')\n",
        "        print('Using latest version.')\n",
        "\n",
        "def past_installation():\n",
        "    return os.path.exists('MDX_Colab')\n",
        "\n",
        "def LoadMDX():\n",
        "    console(f'git clone {ai} MDX_Colab')\n",
        "\n",
        "#-------------------------------------------------------\n",
        "# install requirements\n",
        "print('Installing dependencies will take 45 seconds...',end=' ')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "    svmem = psutil.virtual_memory()\n",
        "    gpu_runtime = False\n",
        "    with h():\n",
        "        console('pip3 install onnxruntime==1.14.1')\n",
        "else:\n",
        "    gpu_runtime = True\n",
        "    with h():\n",
        "        console('pip3 install onnxruntime-gpu==1.14.1')\n",
        "with h():\n",
        "    deps = [\n",
        "            'pathvalidate',\n",
        "            'youtube-dl',\n",
        "            'django'\n",
        "    ]\n",
        "    for dep in deps:\n",
        "        console('pip3 install {}'.format(dep))\n",
        "# import modules\n",
        "#console('pip3 install torch==1.13.1')\n",
        "console('pip3 install soundfile==0.12.1')\n",
        "console('pip3 install librosa==0.9.1')\n",
        "from pathvalidate import sanitize_filename\n",
        "print('done')\n",
        "if not gpu_runtime:\n",
        "    print(f'GPU runtime is disabled. You have {get_size(svmem.total)} RAM.\\nProcessing will be incredibly slow. ðŸ˜ˆ')\n",
        "elif gpu_info.find('Tesla T4') >= 0:\n",
        "    print('You got a Tesla T4 GPU. (speeds are around  10-25 it/s)')\n",
        "elif gpu_info.find('Tesla P4') >= 0:\n",
        "    print('You got a Tesla P4 GPU. (speeds are around  8-22 it/s)')\n",
        "elif gpu_info.find('Tesla K80') >= 0:\n",
        "    print('You got a Tesla K80 GPU. (This is the common gpu, speeds are around 2-10 it/s)')\n",
        "elif gpu_info.find('Tesla P100') >= 0:\n",
        "    print('You got a Tesla P100 GPU. (This is the Second to the fastest gpu, speeds are around  15-42 it/s)')\n",
        "else:\n",
        "    if gpu_runtime:\n",
        "        print('You got an unknown GPU. Please report the GPU you got!')\n",
        "        !nvidia-smi\n",
        "\n",
        "#console('pip3 install demucs')\n",
        "#-------------------------------------------------------\n",
        "# Scripting\n",
        "mount(mount_to_drive)\n",
        "toPath('gdrive' if mount_to_drive else 'local')\n",
        "#check for MDX existence\n",
        "if not past_installation():\n",
        "    print('First time installation will take around 3-6 minutes.\\nThis requires around 2-3 GB Free Gdrive space.\\nPlease try not to interup installation process!!')\n",
        "    print('Downloading AI...',end=' ')\n",
        "    with h():\n",
        "        LoadMDX()\n",
        "    os.chdir('MDX_Colab')\n",
        "    print('done')\n",
        "\n",
        "    print('Downloading models...',end=' ')\n",
        "    with h():\n",
        "        #getDemucs('model/')\n",
        "        getONNX()\n",
        "    if os.path.isfile('onnx_list'):\n",
        "        os.remove('onnx_list')\n",
        "    print('done')\n",
        "\n",
        "else:\n",
        "    os.chdir('MDX_Colab')\n",
        "    update()\n",
        "\n",
        "################\n",
        "#outro\n",
        "print('Success!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "u61Z3hTK5mAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hakZKxmTtaHn"
      },
      "source": [
        "##Audio Isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #Upload your files directly to UVR\n",
        "#@markdown Run this cell to upload your vocal files that you want to use, (or zip files containing audio), to your Colab. <br>\n",
        "#@markdown Alternatively, you can upload from the colab files panel, but this should be more convenient. This method may not work on iOS.\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import display, Javascript\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Create the target directory if it doesn't exist\n",
        "target_dir = '/content/drive/MyDrive/MDX_Colab/tracks'\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    # Check if the uploaded file is a zip file\n",
        "    if fn.endswith('.zip'):\n",
        "        # Write the uploaded zip file to the target directory\n",
        "        zip_path = os.path.join(target_dir, fn)\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(uploaded[fn])\n",
        "\n",
        "        unzip_dir = os.path.join(target_dir, fn[:-4])  # Remove the .zip extension from the folder name\n",
        "\n",
        "        # Extract the zip file\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzip_dir)\n",
        "\n",
        "        # Delete the zip file\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "        print('Zip file \"{name}\" extracted and removed. Files are in: {folder}'.format(name=fn, folder=unzip_dir))\n",
        "\n",
        "        # Display copy path buttons for each extracted file\n",
        "        for extracted_file in os.listdir(unzip_dir):\n",
        "            extracted_file_path = os.path.join(unzip_dir, extracted_file)\n",
        "            extracted_file_length = os.path.getsize(extracted_file_path)\n",
        "\n",
        "            extracted_file_label = widgets.HTML(\n",
        "                value='Extracted file \"{name}\" with length {length} bytes'.format(name=extracted_file, length=extracted_file_length)\n",
        "            )\n",
        "            display(extracted_file_label)\n",
        "\n",
        "            extracted_file_path_text = widgets.HTML(\n",
        "                value='File saved to: <a href=\"{}\" target=\"_blank\">{}</a>'.format(extracted_file_path, extracted_file_path)\n",
        "            )\n",
        "\n",
        "            extracted_copy_button = widgets.Button(description='Copy')\n",
        "            extracted_copy_button_file_path = extracted_file_path  # Make a local copy of the file path\n",
        "\n",
        "            def copy_to_clipboard(b):\n",
        "                js_code = '''\n",
        "                    const el = document.createElement('textarea');\n",
        "                    el.value = \"{path}\";\n",
        "                    el.setAttribute('readonly', '');\n",
        "                    el.style.position = 'absolute';\n",
        "                    el.style.left = '-9999px';\n",
        "                    document.body.appendChild(el);\n",
        "                    el.select();\n",
        "                    document.execCommand('copy');\n",
        "                    document.body.removeChild(el);\n",
        "                '''\n",
        "                display(Javascript(js_code.format(path=extracted_copy_button_file_path)))\n",
        "\n",
        "            extracted_copy_button.on_click(copy_to_clipboard)\n",
        "            display(widgets.HBox([extracted_file_path_text, extracted_copy_button]))\n",
        "\n",
        "        continue\n",
        "\n",
        "    # For non-zip files\n",
        "    # Save the file to the target directory\n",
        "    file_path = os.path.join(target_dir, fn)\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(uploaded[fn])\n",
        "\n",
        "    file_length = len(uploaded[fn])\n",
        "    file_label = widgets.HTML(\n",
        "        value='User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=file_length)\n",
        "    )\n",
        "    display(file_label)\n",
        "\n",
        "    # Check if the uploaded file is a .pth or .index file\n",
        "    if fn.endswith('.pth') or fn.endswith('.index'):\n",
        "        warning_text = widgets.HTML(\n",
        "            value='<b style=\"color: red;\">Warning:</b> You are uploading a model file in the wrong place. Please ensure it is uploaded to the correct location.'\n",
        "        )\n",
        "        display(warning_text)\n",
        "\n",
        "    # Create a clickable path with copy button\n",
        "    file_path_text = widgets.HTML(\n",
        "        value='File saved to: <a href=\"{}\" target=\"_blank\">{}</a>'.format(file_path, file_path)\n",
        "    )\n",
        "\n",
        "    copy_button = widgets.Button(description='Copy')\n",
        "    copy_button_file_path = file_path  # Make a local copy of the file path\n",
        "\n",
        "    def copy_to_clipboard(b):\n",
        "        js_code = '''\n",
        "            const el = document.createElement('textarea');\n",
        "            el.value = \"{path}\";\n",
        "            el.setAttribute('readonly', '');\n",
        "            el.style.position = 'absolute';\n",
        "            el.style.left = '-9999px';\n",
        "            document.body.appendChild(el);\n",
        "            el.select();\n",
        "            document.execCommand('copy');\n",
        "            document.body.removeChild(el);\n",
        "        '''\n",
        "        display(Javascript(js_code.format(path=copy_button_file_path)))\n",
        "\n",
        "    copy_button.on_click(copy_to_clipboard)\n",
        "    display(widgets.HBox([file_path_text, copy_button]))\n",
        "\n",
        "# Remove the original uploaded files from /content/\n",
        "for fn in uploaded.keys():\n",
        "    if os.path.exists(os.path.join(\"/content/\", fn)):\n",
        "        os.remove(os.path.join(\"/content/\", fn))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "xNWcQ6FjVMgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS5gkRxlj-2B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Print a list of tracks\n",
        "for i in glob.glob('tracks/*'):\n",
        "    print(os.path.basename(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHIQxwkUtSDa",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "if not 'initialised' in globals():\n",
        "    raise NameError('Please run the first cell first!! #scrollTo=H_cTbwhVq4K6')\n",
        "\n",
        "#import all models metadata\n",
        "import json\n",
        "with open('model_data.json', 'r') as f:\n",
        "  model_data = json.load(f)\n",
        "\n",
        "# Modifiable variables\n",
        "tracks_path = 'tracks/'\n",
        "separated_path = 'separated/'\n",
        "\n",
        "#@markdown ### Input track\n",
        "#@markdown Enter any link/Filename (Upload your songs in tracks folder)\n",
        "track = \"Eminem.wav\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Models\n",
        "ONNX = \"MDX-UVR Vocal Model Voc_FT\" #@param [\"off\", \"Karokee\", \"Karokee 2\", \"Karokee_AGGR\", \"Kim ft other instrumental model\", \"MDX-UVR Ins Model 415\", \"MDX-UVR Ins Model 418\", \"MDX-UVR Ins Model 464\", \"MDX-UVR Ins Model 496 - inst main-MDX 2.1\", \"MDX-UVR Vocal Model 406\", \"MDX-UVR Ins Model Full Band 292\", \"MDX-UVR Ins Model Full Band 403\", \"MDX-UVR Ins Model Full Band 450 (HQ_1)\", \"MDX-UVR Ins Model Full Band 498 (HQ_2)\", \"MDX-UVR Vocal Model 427\", \"MDX-UVR Vocal Model 9662\", \"MDX-UVR Vocal Model 9682\", \"MDX-UVR Vocal Model 9703\", \"MDX-UVR-Kim Vocal Model (old)\", \"MDX-UVR-Kim Vocal Model 2 (Kim vocal 2)\", \"(de)Reverb HQ By FoxJoy\", \"MDX-UVR Ins Model Full Band new (HQ_3)\", \"MDX-UVR Vocal Model Voc_FT\"]\n",
        "Demucs = 'off'#@param [\"off\",\"demucs_extra\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Parameters\n",
        "denoise = False #@param {type:\"boolean\"}\n",
        "normalise = True #@param {type:\"boolean\"}\n",
        "#getting values from model_data.json related to ONNX var (model folder name)\n",
        "amplitude_compensation = model_data[ONNX][\"compensate\"]\n",
        "dim_f = model_data[ONNX][\"mdx_dim_f_set\"]\n",
        "dim_t = model_data[ONNX][\"mdx_dim_t_set\"]\n",
        "n_fft = model_data[ONNX][\"mdx_n_fft_scale_set\"]\n",
        "\n",
        "mixing_algorithm = 'max_mag' #@param [\"default\",\"min_mag\",\"max_mag\"]\n",
        "chunks = 55 #@param {type:\"slider\", min:1, max:55, step:1}\n",
        "shifts = 10 #@param {type:\"slider\", min:0, max:10, step:0.1}\n",
        "\n",
        "##validate values\n",
        "track = track if 'http' in track else tracks_path+track\n",
        "normalise = '--normalise' if normalise else ''\n",
        "denoise = '--denoise' if denoise else ''\n",
        "\n",
        "if ONNX == 'off':\n",
        "    pass\n",
        "else:\n",
        "    ONNX = 'onnx/'+ONNX\n",
        "if Demucs == 'off':\n",
        "    pass\n",
        "else:\n",
        "    Demucs = 'model/'+Demucs+'.th'\n",
        "#@markdown ---\n",
        "#@markdown ### Stems\n",
        "bass = False #@param {type:\"boolean\"}\n",
        "drums = False #@param {type:\"boolean\"}\n",
        "others = False #@param {type:\"boolean\"}\n",
        "vocals = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Invert stems to mixture\n",
        "invert_bass = False #@param {type:\"boolean\"}\n",
        "invert_drums = False #@param {type:\"boolean\"}\n",
        "invert_others = False #@param {type:\"boolean\"}\n",
        "invert_vocals = True #@param {type:\"boolean\"}\n",
        "invert_stems = []\n",
        "stems = []\n",
        "if bass:\n",
        "    stems.append('b')\n",
        "if drums:\n",
        "    stems.append('d')\n",
        "if others:\n",
        "    stems.append('o')\n",
        "if vocals:\n",
        "    stems.append('v')\n",
        "\n",
        "invert_stems = []\n",
        "if invert_bass:\n",
        "    invert_stems.append('b')\n",
        "if invert_drums:\n",
        "    invert_stems.append('d')\n",
        "if invert_others:\n",
        "    invert_stems.append('o')\n",
        "if invert_vocals:\n",
        "    invert_stems.append('v')\n",
        "\n",
        "margin = 44100\n",
        "\n",
        "###\n",
        "# incompatibilities\n",
        "###\n",
        "\n",
        "console(f\"python main.py --n_fft {n_fft} --dim_f {dim_f} --dim_t {dim_t} --margin {margin} -i \\\"{track}\\\" --mixing {mixing_algorithm} --onnx \\\"{ONNX}\\\" --model {Demucs} --shifts {round(shifts)} --stems {''.join(stems)} --invert {''.join(invert_stems)} --chunks {chunks} --compensate {amplitude_compensation} {normalise} {denoise}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<sup>Models provided are from [Kuielab](https://github.com/kuielab/mdx-net-submission/), [UVR](https://github.com/Anjok07/ultimatevocalremovergui/) and [Kim](https://github.com/KimberleyJensen/) <br> (you can support UVR [here](https://www.buymeacoffee.com/uvr5/vip-model-download-instructions) and [here](https://boosty.to/uvr)).</sup></br>\n",
        "<sup>Original UVR notebook by [Audio Hacker](https://www.youtube.com/channel/UC0NiSV1jLMH-9E09wiDVFYw/), modified by Audio Separation community & then kalomaze (for RVC colab).</sup></br>\n",
        "<sup>Big thanks to the [Audio Separation Discord](https://discord.gg/zeYU2Wzbgj) for helping me implement this in the colab.</sup></br>"
      ],
      "metadata": {
        "id": "z_DD960nGMdi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOmAsaSTha39"
      },
      "source": [
        "##**UVR Colab Settings explanation**<br>\n",
        "\n",
        "* **Mixing algorithm**: \"max_mag\" is for vocals and \"min_mag\" is for instrumentals. \"Default\" is a balance between the two and is required for Demucs enabled with vocal models.\n",
        "* **Chunks**: Set it to 55 or 40 for less aggressive instrument disappearance. Set it to 1 for the best clarity. Use 10 for Demucs enabled or vocal models with tracks below 5:00 minutes. Higher numbers increase separation speed but may cause memory allocation errors.\n",
        "* **Shifts**: Use a maximum of 10 for a slight increase in quality, but note that it significantly increases processing time. Shifts 5 give similar results.\n",
        "* **Normalize**: Normalizes input and improves the separation process by reducing noise. Disable it if you have noisy hi-hats or large amplitude compensation."
      ]
    }
  ]
}
